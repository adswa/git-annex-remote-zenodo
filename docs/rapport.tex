% Created 2021-07-19 lun. 17:45
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Oumaima Hajji}
\date{\today}
\title{rapport stage}
\hypersetup{
 pdfauthor={Oumaima Hajji},
 pdftitle={rapport stage},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.2 (Org mode 9.4.4)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents




\section{Plan (17.5 / 20 pages).}
\label{sec:orgdaf812f}
\subsection{Focalisation (2.5 / 3 pages).}
\label{sec:orgd803559}
\subsubsection{points (2 pages).}
\label{sec:org14f3fc4}
\begin{itemize}
\item En France, il y a un mouvement autour de la science ouverte (def v)
\url{https://www.enseignementsup-recherche.gouv.fr/cid132529/le-plan-national-pour-la-science-ouverte-les-resultats-de-la-recherche-scientifique-ouverts-a-tous-sans-entrave-sans-delai-sans-paiement.html}
\item la science ouverte: "c’est la diffusion sans entrave des
publications et des données de la recherche.
Son objectif : faire sortir la recherche financée sur fonds
publics du cadre confiné des bases de données fermées. Elle réduit
les efforts dupliqués dans la collecte, la création, le transfert
et la réutilisation du matériel scientifique. Elle augmente ainsi
l’efficacité de la recherche."
\item Le plan national pour la science ouverte: "rend obligatoire
l’accès ouvert pour les publications et pour les données issues de
recherches financées sur projets. Il met en place un Comité pour
la science ouverte et soutient des initiatives majeures de
structuration du paysage concernant les publications et les
données."
\item open science, open peer review, open protocol (mooc 1: focalisé
sur la prise des notes, le document computationnal), open data.
("FAIR is not equal to Open: The 'A' in FAIR stands for 'Accessible
under well defined conditions'. \ldots{} As such, while FAIR data does
not need to be open, in order to comply with the condition of
reusability, FAIR data are required to have a clear, preferably
machine readable, license.":
\url{https://www.go-fair.org/resources/faq/ask-question-difference-fair-data-open-data/})
\item FAIR data: \url{https://www.go-fair.org/fair-principles/}
\item on sait qu'il faut faire ça (respecter les principes de FAIR data)
mais comment le faire -> mooc (grâce à les outils donnés).
\item Cette version du mooc se focalise sur la partie archivage data
pour s'assurer de sa disponibilité, \ldots{}
\end{itemize}

\subsubsection{liens.}
\label{sec:orgd67ab9a}
\begin{itemize}
\item les principes de FAIR data: \url{https://www.go-fair.org/fair-principles/}
\item open science: \url{https://fr.wikipedia.org/wiki/Science\_ouverte}
"La science ouverte (open science ou open research pour les
anglophones) est un mouvement qui cherche à rendre la recherche
scientifique et les données qu'elle produit accessibles à tous et
dans tous les niveaux de la société."
\item open peer review:
\url{https://openscience.pasteur.fr/2020/06/04/open-peer-review-un-mouvement-qui-prend-de-lampleur/}
\item open protocol.
\item open data: \url{https://fr.wikipedia.org/wiki/Donn\%C3\%A9es\_ouvertes}
\end{itemize}

\subsection{Etat de l'art (3.5 / 4 pages).}
\label{sec:org2b974d2}
\subsubsection{historique / gros fichiers (1.5 / 2 pages).}
\label{sec:org24380cb}
\begin{enumerate}
\item points importants à inclure.
\label{sec:org57c52a5}
\begin{itemize}
\item Dans la science moderne (science compitationnelle / data
science) , on travaille sur des milliers de
fichiers de données qui évoluent au cours du temps. Il faut donc
garder une trace de chaque version de données et du code (*)
exécuté à un instant pour pouvoir avoir une empreinte continue du
développement de la recherche et donc pour comprendre et suivre
tous les pas de l'expérience. C'est pour cette raison que l'on se sert
de Git qui est le logiciel le plus utilisé pour la gestion de
versions.
changer l'ordre.
\item Puisque c'est difficile de gérer les fichiers de grandes tailles
sur Git on va se servir de git-annex qui fait ça sans
enregistrer le contenu des fichiers sur Git. En effet, blablabla.
\item Mais git-annex n'est pas le seul outil existant qui permet la
gestion des fichiers des gros fichiers. En effet, Git a une
extension qui permet de faire juste ça: Git
lfs. (\url{https://www.atlassian.com/git/tutorials/git-lfs}). Mais il
y a des problèmes avec cet outil qui gênent son utilisation:
expliquer les deux problèmes..
\item Le choix entre les deux outils est donc fait: git-annex!
\end{itemize}
\item extra notes.
\label{sec:org98e0f20}
\begin{enumerate}
\item Importance of Version Controlling Data.
\label{sec:orge07675a}
The reality is that data is only rarely invariant. For example,
throughout a scientific project, datasets can be extended with new
data, adapted to new naming schemes, reorganised into different
file hierarchies, updated with new data points or modified to fix
any errors.
If a dataset that is the basis for computing a scientific result
changes without version control, reproducibility can be threatened:
results may become invalid, or scripts that are based on file names
that change between versions can break.
Therefore, version controlling data and other large files in a
similar way to version controlling code or manuscripts can help
ensure the reproducibility of a project and capture the provenance
of results.
(*) L'importance de 'Data Version Control' est évidente puisque dans
le domaine de la recherche on gère des données de différents types
et de grande taille et ces données peuvent changer et évoluer
rapidement et donc au final on aura plusieurs version de chaque
fichier et quand on compile les fichiers sources on peut avoir des
différents résultats si on prend les mauvaises versions des
fichiers. Mais on peut pas toujours tout mettre sur Git (surtout
dans le cas des fichiers de grande taille qui évoluent
exponentiellement). On gère ça alors with git-annex qui crée un
directory annex où sont stockés les noms et les métadata des
fichiers. Donc quand le dépôt est push sur Github, seuls les
métadata sont transmises et alors les dépôts ne sont plus
lourds. Les données peuvent être transmises sur des autres depôts
(figshare, amazon, \ldots{}) et on peut facilement les récupérer avec
une commande.

\item Challenges in version controlling data.
\label{sec:orgc970780}
When you work, share, and collaborate on large, potentially binary
files (such as many scientific data formats), you need to think
about ways to version control this data with specialised
tools. This is because most version control tools - such as Git -
are not well suited to handle large binary data. As a Git
repository stores every version of every file that is added to it,
large files that undergo regular modifications can inflate the size
of a project significantly. If others try to clone your repository
or fetch/pull to update it locally, it will take longer to do this
if it contains larger files that have been versioned and modified.
\item Version controlling data with git-annex.
\label{sec:org37c6d14}
The git-annex tool is a distributed system that can manage and
share large files independent from a central service or
server. git-annex manages all file content in a separate directory
in the repository (.git/annex/objects, the so-called annex) and
only places file names with some metadata into version control by
Git. When a Git repository with an annex is pushed to a web-hosting
service such as GitHub, the contents stored in the annex are not
uploaded. Instead, they can be pushed to a storage system.
If a repository with an annex is cloned, the clone will not
contain the contents of all annexed files by default, but display
only file names. This makes the repository small, even if it tracks
hundreds of gigabytes of data, and cloning fast, while file
contents are stored in one or more free or commercial external
storage solutions.
\url{https://the-turing-way.netlify.app/reproducible-research/vcs/vcs-data.html}
\end{enumerate}
\end{enumerate}

\subsubsection{archivage (2 pages).}
\label{sec:org2e33884}
\begin{enumerate}
\item points importants à inclure.
\label{sec:orgaf862f7}
\begin{itemize}
\item archivage: pourquoi (pérénité, mise à disposition à une communauté à
définir) et ce que ça fourni (référencement pérène)
\item Exemples: zenodo, nakala, figshare, \ldots{}
\item Chainon manquant dans le workflow de gestion de données: l'archivage
est une opération manuelle.
\item Ébauches de solution:  github-zenodo, gitlab-zenodo
\item Ma proposition: proposer un remote zenodo
\end{itemize}


\begin{itemize}
\item Maintenant que l'on sait comment gérer les fichiers il faut
passer à l'autre étape importante dans ce procès qui est l'étape
de l'archivage !!
\end{itemize}
\begin{itemize}
\item Expliquer que le principe de special remotes est intéressant
puisque la gestion est déjà faite par git-annex et qu'il faut
juste choisir l'un des remotes qui nous est pratique et après on
peut stocker les données dessus.
\item Expliquer pourquoi on n'a pas choisi les special remotes qui
sont déjà implémenté par git-annex:
\url{https://git-annex.branchable.com/special\_remotes/}
\item Expliquer que la solution la plus évidente est de se servir de
ce principe pour implémenter un special remote qui répond à nos
attentes. Mais pour faire cela il y a plusieurs plateformes
d'archivage: zenodo(cern), figshare, nakala, .. Il faut faire une
comparaison de ces outils pour arriver à la conclusion que
Zenodo est l'outil le plus intéressant pour nous.
\item Mais quand on s'appuie sur Zenodo pour faire de l'archivage, on
remarque que il y a un shortcut entre Zenodo et github où les deux
comptes de l'utilisateur sont connectés pour lui permettre
d'upload ses projets github directement sur Zenodo et de les
archiver facilement. Pourquoi pas juste utiliser ce shortcut au
lieu de passer par git et git-annex? Le problème c'est que ce
mechanisme est personnalisé juste pour github et donc on ne peut
pas faire cela avec des autres plateformes comme gitlab sans
devoir passer par des biblio. Et même quand on fait ça, il y a
toujours un problème avec le lien Zenodo-Gitlab car cette
méthode permet juste d'upload des fichiers sur Zenodo en
utilisant l'API et ne permet pas de faire plus que ça. Donc la
solution la plus évidente est de commencer par git et de
construire un chemin vers Zenodo.
\item Parler de datalad et comme quoi il y a aussi un problem là car a
seule solution d'archivage de ce type proposée par datalad est
d'upload des archive zip sur figshare. donc on a implémenté le
remote zenodo pour faire ça.
\end{itemize}

\item extra notes.
\label{sec:org4b0a97c}
\begin{enumerate}
\item zenodo.
\label{sec:orgcb18165}
Zenodo is a general-purpose open-access repository developed under
the European OpenAIRE program and operated by CERN. It allows
researchers to deposit research papers, data sets, research
software, reports, and any other research related digital artifacts.
\begin{itemize}
\item We will be using Zenodo as the database where the articles and
research papers will be deposited at the end of the mooc. The API
is easily accessible through Python with the use of the package
requests which allows the use of the basic HTTP queries.
\end{itemize}
\item datalad.
\label{sec:org690f26b}
\begin{itemize}
\item DataLad builds on top of git-annex and extends it with an
intuitive command-line interface. It enables users to operate
on data using familiar concepts, such as files and directories,
while transparently managing data access and authorization with
underlying hosting providers.
A powerful and complete Python API is also provided to enable
authors of data-centric applications to bring versioning and the
fearless acquisition of data into continuous integration workflows.
\item Converting an existing Git or git-annex repository into a
DataLad dataset: 	\$ datalad create -f
\item DataLad only cares (knows) about two things: Datasets and
files. A DataLad dataset is a collection of files in
folders. And a file is the smallest unit any dataset can
contain. Thus, a DataLad dataset has the same structure as any
directory on your computer, and DataLad itself can be
conceptualized as a content-management system that operates on
the units of files.
\item exporting the content of a dataset as a ZIP archive to figshare:
Ideally figshare should be supported as a proper git annex special
remote. Unfortunately, figshare does not support having directories,
and can store only a flat list of files. That makes it impossible
for any sensible publishing of complete datasets.
\end{itemize}
\end{enumerate}
\end{enumerate}
\subsubsection{liens.}
\label{sec:org8c97820}
\begin{itemize}
\item comparing the archiving platforms: \url{https://espacechercheurs.enpc.fr/fr/donnees-recherche-aspects-techniques}
\item git-annex vs lfs: \url{https://stackoverflow.com/questions/39337586/how-do-git-lfs-and-git-annex-differ}
\item nakala: \url{https://documentation.huma-num.fr/nakala/\#introduction-et-presentation}
\item mendeley: \url{https://data.mendeley.com/archive-process}
\item datalad.
\item figshare.
\item github to zenodo: we know that there is alink between the two which allows to archive a github repository on zenodo (this is especially useful in the case of  when a researcher wants to cite the findings they have on github but they don't have the doi, so the next step to do is to use zenodo to archive the files that are on this repository and so we get at the end the doi number which allows us to cite): \url{https://guides.github.com/activities/citable-code/}
\item l'archivage gitlab -> zenodo ne gère pas les fichiers dans git LFS: \url{https://gitlab.com/lnesi/icpp21/-/jobs/1430800588}
\item library allowing to archive from gitlab to zenodo. It's still in beta stages and has just been developped since there isn't one that is already there like the github direct link: \url{https://pypi.org/project/gitlab2zenodo/}
\end{itemize}
\url{https://gitlab.com/gitlab-org/gitlab/-/issues/25587}
\url{https://github.com/zenodo/zenodo/issues/1404} !!
\url{https://gitlab.com/gitlab-org/gitlab/-/issues/18763}
\subsection{Contributions (6.5 / 8 pages).}
\label{sec:orgdaed854}
\subsubsection{modele de donnees (1 / 1.5 page).}
\label{sec:orgb1e2111}

\begin{itemize}
\item Même si Zenodo paraît comme une la plateforme parfaite à utiliser comme un
special remote de git-annex, il y a toujours un problème
architecturel qui nous a gêné quand on a commencé la réfléxion de
comment structurer notre remote. Quand on fait un upload Zenodo,
puisque son infrastructure ne permet pas d'avoir des directory,
alors le stockage se fait dans une liste des fichiers mal structurée.
\item Un autre problème est aussi le fait que dans Zenodo quand on crée un
nouveau upload, c'est toujours un dépôt où on va mettre tous nos
fichiers. Donc, on peut choisir des différents modèles
d'implémentation du remote Zenodo et pour chaque modèle, la
fréquence de création des dépôts et les fichiers qui sont dedans
changent. Si un dépôt est créé au moment de l'initialisation du
remote, alors toutes les opérations qui viennent seront appliquées
sur ce dépôt, et on aura ainsi un seul dépôt pour chaque remote
créé. Mais on fait un autre choix, où l'initialisation du remote ne
déclenche pas la création du dépôt, et au lieu faire cela après,
alors c'est possible d'avoir plusieurs dépôts, et alors plusieurs
identificateurs de dépôts pour un seul remote git-annex. Or, cela
n'est pas nécéssaire puisque l'on peut choisir les fichiers à mettre
dans un dépôt et ceux à laisser en local, et donc avoir un seul
dépôt par directory (l'endroit où on initialise le remote) est
suffisant puisqu'on peut l'utiliser quand on veut pour manipuler les
fichiers que l'on veut sans avoir des problèmes de confusion. Si
l'utilisateur veut créer un autre dépôt Zenodo avec un remote
git-annex dans le même endroit que le premier remote, c'est toujours
possible d'initialiser plusieurs remotes git-annex dans la même
directory.
\item On a aussi fait des tests pour voir s'il y a des limites imposées
sur le nombre de fichiers possibles à mattre dans un dépôt mais il
n'y avait pas des problèmes avec ces tests et donc c'est possible
d'uplad des milliers de fichiers mais la taille du dépôt ne doit pas
atteindre 50GB. C'est la seule limite imposée par Zenodo. On les a
contacté pour s'assurer de cette hypothèse et on a eu une réponse positive.
\end{itemize}




\subsubsection{Implem de remote zenodo (3.5 / 4.5 pages)}
\label{sec:org255f1ad}
L'implémentation du remote Zenodo s'est faite en plusieurs étapes:
\begin{enumerate}
\item api rest (0.5 page).
\label{sec:org73127b3}
\texttt{new\_version} ou \texttt{publish}

un \emph{record} ou un truc important

\begin{description}
\item[{nouveau dépot}] 

\item[{publish}] 
\end{description}

ou alors \textbf{new\textsubscript{record}} et \textbf{publish}


la première partie du procès est de comprendre comment fonctionne
l'API Zenodo et de tester les fonctionnalités possibles de cette
API. Il fallait faire des tests pour chacune des requêtes HTTP pour
tester les opérations possibles Zenodo. Les opérations les plus
importantes comme la création du dépôt, l'envoi des fichiers sur le
dépôt, la suppression, et l'obtention des informations sur le dépôt et
les fichiers stockés dedans. Il y a aussi des autres opérations pour
publier le dépôt pour archiver les données (une fois un dépôt est
publié, il devient un record qui a un doi et que l'on peut citer alors
quand on veut, on ne peut donc pas supprimer un record une fois
publié. C'est comme ça que l'on garantie son existence et on le rend
accessible et trouvable depuis le doi). On peut aussi créer des
nouvelles versions d'un record avec une simple requête post vers
l'API, et c'est grâce à cette opération que l'on veut faire évoluer
ses données en gardant des traces (chaque version publiée d'un record
a son doi, et puisque l'on peut juste avoir une seule nouvelle version
à la fois, on peut s'assurer du développement des données).
\item biblio python qui implemente deja le protocol (0.5 page).
\label{sec:org091e491}
\begin{itemize}
\item Afin d'implémenter un remote git-annex\footnote{Le protocole \texttt{git-annex} \url{http://git-annex.org/protocol}} il
faut d'abord être sûr que son. \cite{zenodo}
\end{itemize}
program implémente bien le protocole 'external special remote' de
git-annex qui fait le lien entre git-annex et son remote externe. Les
deux bout de la communication échangent des requêtes et des réponses
durant la période de l'exécution du programme
(celui qui implémente le remote X: git-annex-remote-X). Pour ne pas
avoir des soucis de confusion des intéractions, à chaque fois l'une des
deux parties prend l'initiative en n'envoyant que des requêtes et
l'autre partie répond alors avec des reponses à ces requêtes.
\begin{itemize}
\item On utilise la bibliothèque \textbf{AnnexRemote} de python qui implémente la
totalité du protocole et respecte toutes ses spécifications. Il faut
donc juste importer cette les modèles de cette bibiliothèque que
l'on veut utiliser dans notre programme. On définit alors une classe
pour notre remote qui extend la classe SpecialRemote de la
bibliothèque. Ensuite, il nous reste à implémenter les fonctions que
l'on va utiliser pour déposer les données sur le remote et les
manipuler.
\end{itemize}

\item operations principales (1.5 / 2 pages).
\label{sec:org84c71d4}
Chaque remote Zenodo doit être capable d'exécuter des opérations
principales qui servent à envoyer les fichiers sur le remote, les
manipuler, et les récupérer en local. Tout cela se fait avec les
fonctions du programme principal git-annex-remote-zenodo avec 
creation d'un depot, upload, check, remove, get.
\item tests (0.5 page).
\label{sec:org151a078}
avec les exceptions du protocol pour s'assurer que les pb de l'api
passent bien à git-annex et qu'il y a une coherence en les deux .
debug mode
\end{enumerate}
\subsubsection{archiver avec disableremote (1 page)}
\label{sec:org4f3fe05}
Quand la première partie de la gestion des données finit, et on stocke
tous les fichiers qui nous intéressent dans le remote, il faut
maintenant passer à la deuxième partie de l'archivage qui se fait
indépendamment de la première, et où on finalise son dépôt avec toutes
les méta-données nécessaires avant de le publier.
\begin{itemize}
\item les options pour par exemple publier le fichier .json ou
\end{itemize}
\begin{enumerate}
\item newversion.
\label{sec:orga088b16}
\end{enumerate}
\subsubsection{restaurer une archive (1 page)}
\label{sec:org28508bd}

\subsection{Evaluation (1 page).}
\label{sec:org09c4325}
rédaction d'un tutorial pour tester et comprendre la totalité du projet (0.5 page).
\begin{itemize}
\item walkthrough, intégration au MOOC 2 en cours
\end{itemize}
\subsection{Méthodologie et Compétences développées (3 pages).}
\label{sec:orga27841d}
\subsubsection{comppétences développées (1 page).}
\label{sec:org7d59743}
\begin{itemize}
\item Bilan des connaissances et expériences acquises ou approfondies au
\end{itemize}
cours de ce stage.
\begin{itemize}
\item Description sur une page d'une ou deux compétences développées
pendant le stage. Cela peut être des compétences du métier
d'ingénieur en informatique ou aussi des compétences
transversales au métier d'ingénieur (voir les deux fichiers excel
attachés).
\item MOOC 1 (prise des notes! rendre mon travail compréhensible par
quelqu'un d'autre)
\end{itemize}

\subsubsection{méthodologie (2 page).}
\label{sec:orga69acbe}
\begin{enumerate}
\item documentaion de l'ensemble du processus (0.5 page).
\label{sec:orgefa5e29}
\begin{itemize}
\item ex: parler du journal (application directe des éléments de la RR).

\begin{itemize}
\item parler des tutos faits au début / des petits programmes écrits
pour tester les outils (API Zenodo, tuto git-annex, tuto
snakemake?)
\end{itemize}
\item tests
\begin{itemize}
\item doc
\end{itemize}
\end{itemize}
\item Gestion du projet:
\label{sec:org8f03be2}
Description de la gestion de votre projet
     (cycle de vie, structuration en taches, durées estimées et
     réelles, gestion de risques …)
\begin{itemize}
\item reunions hebdomadaires, agile,
\item gantt chart, a posteriori, planification légère
\item identifier mes propositions personnelles (que ce soit des
trouvailles ou des directions à ne pas emprunter)
\end{itemize}
\end{enumerate}

\subsection{Conclusion (1 - 2 page).}
\label{sec:org3ea791c}
ce j'ai pas pu faire: nakala - datalad (submodules ) voir comment ça
peut s'integrer avec zenodo (ex de figshare par opposition) -
(snakemake <-> git-annex) : pb: où integrer les commandes git annex
simples (ex get) dans un workflow snakemake.

\subsection{Bibliographie.}
\label{sec:org6858bc7}
liens à mettre après.
\end{document}