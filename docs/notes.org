
* Introduction to the tools that will be used.
** git-annex.
*** intro.
   git-annex allows managing files with git, without checking the
   filecontents into git. While that may seem paradoxical, it is
   useful when dealing with files larger than git can currently
   easily handle, whether due to limitations in memory, time, or disk
   space.
   - In our framework, it will be used to keep hold of the multiple
     versions of the files that are used during the process of research
     and data analysis. It's imperative to keep track of the versions so
     as to facilitate the task when reproducing a result.
     (in this case most files are large and git-annex is the best at
     handling the size).
*** useful links.
   https://git-annex.branchable.com/walkthrough/
   https://git-annex.branchable.com/special_remotes/external/


** Zenodo.
*** intro.
    Zenodo is a general-purpose open-access repository developed under
    the European OpenAIRE program and operated by CERN. It allows
    researchers to deposit research papers, data sets, research
    software, reports, and any other research related digital
    artifacts.
    - We will be using Zenodo as the database where the articles and
      research papers will be deposited at the end of the mooc. The API
      is easily accessible through Python with the use of the package
      requests which allows the use of the basic HTTP queries.
*** useful links.
    https://developers.zenodo.org/#quickstart-upload

    
** Datalad.
*** intro.
    DataLad builds on top of git-annex and extends it with an
    intuitive command-line interface. It enables users to operate
    on data using familiar concepts, such as files and directories,
    while transparently managing data access and authorization with
    underlying hosting providers.
    A powerful and complete Python API is also provided to enable
    authors of data-centric applications to bring versioning and the
    fearless acquisition of data into continuous integration workflows.

*** additional functionalities to Git / git-annex.
    - minimize the use of unique/idiosyncratic functionality.
    - simplify working with repositories.
    - add a range of useful concepts and functions.
    - make the boundaries between repositories vanish from a user’s
      point of view.
    - provide users with the ability to act on “virtual” file paths.

*** key points.
    - DataLad manages your data, but it does not host it.
    - You can make DataLad publish file content to one location
      and afterwards automatically push an update to GitHub.
    - DataLad scales to large dataset sizes.
    - dataset = superdataset = subdataset unless it's registered
      in another dataset: sub / super.
    - Converting an existing Git or git-annex repository into a
      DataLad dataset: 	$ datalad create -f
    - A dataset is a Git repository. All features of the version
      control system Git also apply to everything managed by DataLad.
    - DataLad only cares (knows) about two things: Datasets and
      files. A DataLad dataset is a collection of files in
      folders. And a file is the smallest unit any dataset can
      contain. Thus, a DataLad dataset has the same structure as any
      directory on your computer, and DataLad itself can be
      conceptualized as a content-management system that operates on
      the units of files.
    - A DataLad dataset can take care of managing and version
      controlling arbitrarily large data. To do this, it has an optional
      annex for (large) file content.
    - Deep in the core of DataLad lies the social principle to
      minimize custom procedures and data structures. DataLad will not
      transform your files into something that only DataLad or a
      specialized tool can read. A PDF file (or any other type of
      file) stays a PDF file (or whatever other type of file it was)
      whether it is managed by DataLad or not. This guarantees that
      users will not lose data or access if DataLad would vanish from
      their system.
    - DataLad allows to capture full provenance: The origin of
      datasets, the origin of files obtained from web sources,
      complete machine-readable and automatically reproducible records
      of how files were created (including software environments).
      
      
*** exporting the content of a dataset as a ZIP archive to figshare.
    Ideally figshare should be supported as a proper git annex
    special remote. Unfortunately, figshare does not support having
    directories, and can store only a flat list of files. That makes
    it impossible for any sensible publishing of complete datasets.
    $ datalad export-to-figshare [-h] [-d DATASET] [--missing-content {error|continue|ignore}] [--no-annex] [--article-id ID] [PATH]	(*)

*** useful links.
    https://handbook.datalad.org/en/latest/basics/101-180-FAQ.html
    http://docs.datalad.org/en/stable/generated/man/datalad-export-to-figshare.html	 (*)
    http://handbook.datalad.org/en/latest/usecases/ml-analysis.html
    https://handbook.datalad.org/en/latest/beyond_basics/101-168-dvc.html
    http://docs.datalad.org/en/stable/generated/datalad.api.Dataset.html
    http://docs.datalad.org/en/stable/generated/man/datalad-export-archive-ora.html
    https://carpentries.topicbox.com/groups/discuss/Tb776978a905c0bf8-M3d3e4bb2f0a49fdf2391282c 
    http://handbook.datalad.org/en/latest/index.html

    
** Snakemake.
*** intro.
    Snakemake is a workflow engine that provides a readable
    Python-based workflow definition language and a powerful
    execution environment that scales from single-core workstations
    to compute clusters without modifying the workflow.
    - As projects grow or age, it becomes increasingly difficult to
      keep track of all the parts and how they fit together.
    - This will be used to handle the workflows in our project and
      and it's useful in the case of reproducibility because it allows
      the definition and the use of environments. For instance, we can
      define a seperate conda env in a rule in the Snakefile and
      we can inculde the packages we want to use while configurating
      the new conda environment (command: snakemake --use-env).
      
*** useful links.
    https://snakemake.readthedocs.io/en/stable/
    https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html#tutorial
    https://www.youtube.com/watch?v=NNPBDOBHlxo&ab_channel=EdinburghGenomicsTraining
    https://snakemake.readthedocs.io/en/stable/snakefiles/deployment.html

    
** GUIX.
*** intro.
   "GNU GUIX is a general-purpose package manager that implements 
   the functional package management paradigm pioneered by Nix" (*). it
   allows the creation and the deployment of isolated software
   environments, operating systems and the maintaining of a large
   range of packages.
   - By definition, this software is a great addition to our arsenal
     in the subject of reproducible research. And among the many uses it
     has, tools like time-machine, for instance, allow the execution of
     old programmes. By finding the right version of the commit and
     installing these versions of dependencies, we can get back to a
     similar state to the old one and so, by executing the programme, we
     can get the same results.
*** useful links.
    https://link.springer.com/content/pdf/10.1007%2F978-3-319-27308-2_47.pdf (*)
    https://guix.gnu.org/manual/en/html_node/Binary-Installation.html
    https://guix.gnu.org/en/manual/en/guix.html

    
** Docker.
*** intro.
    Docker is a software for developing, shipping, and running
    applications. It allows the user to create seperate containers for
    their applications which seperates them from the infrastructure of
    the computer of the user and facilitates their use from other
    individuals.
    - Having a container which isolates the app from the environment
      and where the developper can state the packages and the versions
      that they want to use makes it possible for them to reproduce
      the same results. All they need to do is to create an image
      using a Dockerfile and to store it so as to be used to create a
      container whenever needed.
      Dockerfiles can be tracked and versioned in source control
      repositories. Once a Docker container has been built, its
      contents can be exported to a binary file; these files are
      generally smaller than virtual machine files, so they can be
      shared more easily.
    - The use of Docker containers for reproducible research comes
      with caveats. Individual containers are stored and executed in
      isolation from other containers on the same computer; however,
      because all containers on a given machine share the same
      operating system, this isolation is not as complete as it is
      with virtual machines. This means, for example, that a given
      container is not guaranteed to have access to a specific amount
      of computer memory or processing power—multiple containers may
      have to compete for these resources

*** useful links.
   https://reproducible-analysis-workshop.readthedocs.io/en/latest/8.Intro-Docker.html
   https://gigascience.biomedcentral.com/articles/10.1186/s13742-016-0135-4

   
** Conda
   Conda is an open source package management system and environment
   management system that runs on Windows, macOS and Linux. Conda
   quickly installs, runs and updates packages and their dependencies. 
   Conda provides multiple ways of reproducing project environments.
   Creating a clone of an environment can provide a custom base
   environment or snapshot of the environment. Spec list and
   conda-pack create platform and operating system specific copies of
   an environment. 
   

** Singularity.
   Singularity is a container platform. It allows you to create and
   run containers that package up pieces of software in a way that is
   portable and reproducible.
   - Having a container which isolates the app from the environment
     and where the developper can state the packages and the versions
     that they want to use makes it possible for them to reproduce
     the same results.

     
* The tutorials that were done in order get a better grasp of the tool.
** git-annex.
*** useful commands.
    - creating a repository:
      #+BEGIN_EXAMPLE
      git init 
      git annex init
      #+END_EXAMPLE
    
    - adding files:
      #+BEGIN_EXAMPLE
      git annex add .
      git commit -a -m added
      #+END_EXAMPLE

    - adding a remote (usb drive):
      #+BEGIN_EXAMPLE
      sudo mount /media/usb
      cd /media/usb
      git clone ~/annex
      cd annex
      git annex init "portable USB drive"
      git remote add laptop ~/annex
      cd ~/annex
      git remote add usbdrive /media/usb/annex
      #+END_EXAMPLE

    - getting file content:
      #+BEGIN_EXAMPLE
      cd /media/usb/annex
      git annex sync laptop
      git annex get .
      #+END_EXAMPLE

    - syncing:
      #+BEGIN_EXAMPLE
       cd /media/usb/annex
       git annex sync
      #+END_EXAMPLE

    - removing files:
      #+BEGIN_EXAMPLE
       git annex drop iso/debian.iso
      #+END_EXAMPLE

*** useful links.
    https://git-annex.branchable.com/walkthrough/
    https://git-annex.branchable.com/special_remotes/external/


** Zenodo.
*** Uploading through the API.
**** with cURL:
    - LINK: https://felipecrp.github.io/2021/01/01/uploading-to-zenodo-through-api.html
    - PROCESS:
      + We tested to see if the API is accessible by using a GET
        request and there wasn't an error message.
      + We then sent a POST request to the API to request the creation
	of a new deposit which we will be using to upload our files
	later on. We then receive a JSON message confirming the
        creation of the deposit and extra information (date, title, owner, ..).
      + We then sent a PUT request to upload the files in the deposit
        using the bucket link that was sent in the JSON message when
        we created the deposit.
      + Once we finish uploading the files, we can check the deposit
        to see if they have been uploaded. I did the test twice and
	uploaded simple .txt files (zenodotest.txt & zenodotest2.txt)
	and they are accessible via this URL: https://zenodo.org/api/files/4aefd393-ed38-489c-bc8c-2413d9cb160f/zenodotest2.txt?access_token=WgYPkomVp1HpJniDmwS2ylFBhwsNpntxFzKqo02HGij94nVFlO0tAefboqtn

**** with Python:
    - LINK: https://developers.zenodo.org/?python#quickstart-upload
    - PROCESS:
      + The process is similar to the last one. We first import the
	package requests which we will be using to send the HTTP
	requests to the browser, and then we use a GET request to
	access the deposit (while giving an authentication key with
	the right access). 
      + We then send a POST request to create a new deposit and we
	get in return a message containing information about this
	new deposit (id, links, ..).
      + Now, we can finally create new uploads. We first fetch the
	bucket URL by using this command which retrieves the item
	corresponding to the key "bucket" in the "links" dictionary.
     #+BEGIN_EXAMPLE
bucket_url = r.json()["links"]["bucket"]
      #+END_EXAMPLE

	We use the method PUT to upload out file into the deposit.
	I did that using the new API.
	
*** Uploading through the website:
    I also tested how to publish an article and save it in the
    database. To do this, I simply used the sandbox and uploaded
    a report I worked on with an other student last year.

    
** Snakemake.
   I created a simple Snakefile in which I wrote a couple rules with
   shell commands and tried to compile some old projects in order to
   test this tool. 
   - LINK:
     https://www.youtube.com/watch?v=hPrXcUUp70Y&ab_channel=NCSAatIllinois

     
** Docker.
   In the case of Docker, I've alredy used this tool during the
   project of the semester 8 and so I have a grasp on its basic
   usage. I've already manipulated existing images and deployed
   containers either separately or in groups using docker-compose.
   In the case of creating new images, I've tried creating some using
   simple Dockerfiles.

   
* An introduction to Reproducible Research.
  It is the practice of having the flow of research documented in all
  of its steps in order to get the same result even if it's retested
  by other researchers or in the future. It relies on many components
  of the scientific study such as: having accessible data, detailed
  research and analysis, reproducible workflows and evironments, ..
  - The main features that need to be present for a scientific
    research to be reproductibility-friendly:
    + WF specification: connected tools steps of the analysis.
    + WF execution: provenence modules, data management, ..
    + WF environment: companion tools like Virtual machines, containers, ..
  - Reproducible practices do not necessarily ensure that others can
    obtain identical results to those obtained by the original
    scientists. Indeed, this objective may be infeasible for some
    types of computational analysis, including those that use
    randomization procedures, floating-point operations, or
    specialized computer hardware.

    
* A look into GUIX and reproducible software environments.
** Link:
  https://link.springer.com/content/pdf/10.1007%2F978-3-319-27308-2_47.pdf
  Paper: /Reproducible and User-Controlled Software Environments in HPC
with Guix/ by _Ludovic Courtès_ and _Ricardo Wurmus_.

** Notes.
  - "GNU GUIX is a general-purpose package manager that implements the 
  functional package management paradigm pioneered by Nix".

  - It's important to handle the software environments when trying to
    work on reproducible research because the work that is being done
    mainly focuses on workflows, conventions, ... . We need to have
    the same  software environment to be able to reproduce the same results.
  - A solution that was first given is to either write down the
    numbers of the dependencies (insufficient), or to save/download
    and reuse full system images (the images are large + it's
    difficult to combine with the software environment of each of
    the users). -> GUIX is the sollution to these problems.
  - Reproducing the exact same software environment on a different HPC
    system could allow the users to assess the impact of the hardware
    on the software’s performance + it would allow other researchers
    to reproduce the same experiment on their systems. 
  - Package managers like APT / RPM suffer from limitations:
	+ Package binaries that every user installs, such as .deb
          files, are actually built on the package maintainer’s
	  machine, and details about the host may leak into the
	  binary that is uploaded.
	+ While it is possible for users to define their own variant
	  of a package, it's still difficult to do. For instance,
	  even if a user builds a custom .spec file, they can't always
	  register it in the yumdb database (it's only allowed for
	  the administrator) so it's difficult for the user to track
	  down and register the complete graph of dependencies manually.
	+ These tools implement an imperative and stateful package
	  management model: imperative in the sense that it modifies
	  the set of available packages in place (ex: switching to an
	  alternative MPI implementation, or upgrading the OpenMP
	  run-time library means that suddenly all the installed
	  applications and libraries start using them). It is stateful
	  because the system state at a given point in time is the
	  result of the series of installation and upgrade operations
	  that have been made over time, and there may be no way to
	  reproduce the exact same state elsewhere.

  - Package management is to be seen as functional paradigms where the
    results only depend on the inputs. So, rerunning a given build
    with the same input should result in bit-identical files (used
    by Nix and now by Guix). A tool that is used is chroot which
    allows them to run in a limited environment with a defined set
    of env variables, a dedicated user ID, separate name spaces for
    PIDs, inter-process communication (IPC), networking, ... This is
    so as to ensure that build can't end up using libraries it's not
    supposed to use -> this is what allows this process to be seen as
    pure functions with reproducible results.
  - After each build, the created files are stored in the /gnu/store
    directory. each entry has a name that includes a hash of all the
    inputs of the build process that led to it (libs, compilers, ..).
    Therefore, we can fetch the diagram of dependencies easily in this
    case. After running 'guix build openmpi' for instance, we get as a
    return the directory name /gnu/store/xx-openmpi-1.8.1. and the
    daemon spawns the build process in an isolated environment (if
    the directory doesn't exist). For the normal user, the command
    'guix package' is enough to install the packages without  typing
    out the long names. In fact, this command creates a symbolic link
    to the selected /gnu/store item and the symbolic link is then
    stored in  ~ /.guix-profile.



* Combining Snakemake with other tools.
** Integrated package management with Conda.
   We can now define isolated software environments in each rule and
   deploy them using Conda upon the execution of the workflow.
   Packages will be installed into your working directory, without
   requiring any admin/root priviledges. Snakemake will store the
   environment persistently in .snakemake/conda/$hash with $hash
   being the MD5 hash of the environment definition file content.
   This way, updates to the environment definition are automatically
   detected.
   
** Running jobs in containers.
   As an alternative to using Conda, it is possible to define, for
   each rule, a docker or singularity container to use. It will
   execute the job within a singularity container that is spawned
   from the given image.

** Combining Conda with containers.
   A global definition of a container image can be combined with a
   per-rule conda directive upon using the command:
      #+BEGIN_EXAMPLE
      snakemake --use-conda --use-singularity
      #+END_EXAMPLE
   Snakemake will first pull the defined container image, and then
   create the requested conda environment from within the container.
   The conda environments will still be stored in your working
   environment, such that they don’t have to be recreated unless they
   have changed. The hash under which the environments are stored
   includes the used container image url, such that changes to the
   container image also lead to new environments to be created.
   When a job is executed, Snakemake will first enter the container
   and then activate the conda environment.
   By this, both packages and OS can be easily controlled without the
   overhead of creating and distributing specialized container
   images. It is also possible to define a container image per rule.
   The user can, upon execution, freely choose the desired level of reproducibility:
   - no package management (use whatever is on the system).
   - Conda based package management (use versions defined by the workflow developer).
   - Conda based package management in containerized OS (use versions
     and OS defined by the workflow developer).
   + Link: https://snakemake.readthedocs.io/en/v5.1.4/snakefiles/deployment.html





* Version control for data.
** Importance of Version Controlling Data.
   The reality is that data is only rarely invariant. For example,
   throughout a scientific project, datasets can be extended with new
   data, adapted to new naming schemes, reorganised into different
   file hierarchies, updated with new data points or modified to fix
   any errors.
   If a dataset that is the basis for computing a scientific result
   changes without version control, reproducibility can be threatened:
   results may become invalid, or scripts that are based on file names
   that change between versions can break.
   Therefore, version controlling data and other large files in a
   similar way to version controlling code or manuscripts can help
   ensure the reproducibility of a project and capture the provenance
   of results.

** Challenges in version controlling data.
   When you work, share, and collaborate on large, potentially binary
   files (such as many scientific data formats), you need to think
   about ways to version control this data with specialised
   tools. This is because most version control tools - such as Git -
   are not well suited to handle large binary data. As a Git
   repository stores every version of every file that is added to it,
   large files that undergo regular modifications can inflate the size
   of a·project significantly. If others try to clone your repository
   or fetch/pull to update it locally, it will take longer to do this
   if it contains larger files that have been versioned and modified.

** Version controlling data with git-annex.
   The git-annex tool is a distributed system that can manage and
   share large files independent from a central service or
   server. git-annex manages all file content in a separate directory
   in the repository (.git/annex/objects, the so-called annex) and
   only places file names with some metadata into version control by
   Git. When a Git repository with an annex is pushed to a web-hosting
   service such as GitHub, the contents stored in the annex are not
   uploaded. Instead, they can be pushed to a storage system.
   If a repository with an annex is cloned, the clone will not
   contain the contents of all annexed files by default, but display
   only file names. This makes the repository small, even if it tracks
   hundreds of gigabytes of data, and cloning fast, while file
   contents are stored in one or more free or commercial external
   storage solutions.
      
** Link:
   https://the-turing-way.netlify.app/reproducible-research/vcs/vcs-data.html




* Tutorial git-annex
** Intro.
We can start off by creating a new repository where we will be working.
#+BEGIN_SRC shell :session *shell* :results output :exports both 
cd ~/Desktop
mkdir tutos
cd tutos
mkdir git-annex
cd git-annex
#+END_SRC

Then, we initialize a git repository and we initialize this as a
git-annex repository as well and we can choose a name for it
("testlaptop").
#+BEGIN_SRC shell :session *shell* :results output :exports both 
git init
git annex init "testlaptop"
#+END_SRC

We can see in the results that the repository is initialized and its
state has been recorded by git.
#+RESULTS:
: Initialized empty Git repository in /home/nubudi/Desktop/tutos/git-annex/.git/
: init testlaptop ok
: (recording state in git...)

We now just copy a file we want to work with. I just copied a random
file from my Downloads directory into the current one. I then added
this file into the git annex repository using the command add.
#+BEGIN_SRC shell :session *shell* :results output :exports both 
cp ~/Downloads/AirPollutionInGrenoble.pdf .
git annex add AirPollutionInGrenoble.pdf
#+END_SRC

We can see that the state of the file is being recorded now because we
added  it to git.
#+RESULTS:
: 
: add AirPollutionInGrenoble.pdf ok
: (recording state in git...)

All is left is for us to commit the changes.
#+BEGIN_SRC shell :session *shell* :results output :exports both 
git commit -m "just added a new pdf file (report on air pollution)" 
#+END_SRC

#+RESULTS:
: [master (root-commit) f118046] just added a new pdf file (report on air pollution)
:  1 file changed, 1 insertion(+)
:  create mode 120000 AirPollutionInGrenoble.pdf

The file has now become a symlink pointing to a subdirectory of
./git/annex as we can see.
#+BEGIN_SRC shell :session *shell* :results output :exports both 
ls -lh
#+END_SRC


#+RESULTS:
: total 4,0K
: .git/annex/objects/z9/ZJ/SHA256E-s1107977--5ae39c189f2042e2dddc9db35aed4a3f0b9955c1c1e3fa48205e14a7cce23a84.pdf/SHA256E-s1107977--5ae39c189f2042e2dddc9db35aed4a3f0b9955c1c1e3fa48205e14a7cce23a84.pdf

** Using a USB drive as a second location.
   We start off by creating a new folder where we will be working.
#+BEGIN_SRC shell :session *shell* :results output :exports both 
cd "/media/nubudi/Ubuntu 18.04.3 LTS amd641"
mkdir git-annex-usb
cd git-annex-usb
#+END_SRC

  We then initialize a git and a git annex repository.
#+BEGIN_SRC shell :session *shell* :results output :exports both 
git init
git annex init "testusb"
#+END_SRC

#+RESULTS:
#+begin_example
Initialized empty Git repository in /media/nubudi/Ubuntu 18.04.3 LTS amd641/.git/
init testusb 
  Detected a filesystem without fifo support.

  Disabling ssh connection caching.

  Detected a crippled filesystem.

  Enabling direct mode.
ok
(recording state in git...)
#+end_example

  We add the repository we have just created as a remote in the first
  one. This is done so that the two locations know about each other.
#+BEGIN_SRC shell :session *shell* :results output :exports both 
cd ~/Desktop/tutos/git-annex
git remote add usb "/media/nubudi/Ubuntu 18.04.3 LTS amd641/git-annex-usb"
#+END_SRC

  We do the same, adding the laptop repository as a remote in the usb
  repository. 
#+BEGIN_SRC shell :session *shell* :results output :exports both 
cd "/media/nubudi/Ubuntu 18.04.3 LTS amd641/git-annex-usb"
git remote add testlaptop ~/Desktop/tutos/git-annex
#+END_SRC

  Now tha the two repositories are connected and know about each
  other, we can use the command sync to synchronize the git state
  between the two.
#+BEGIN_SRC shell :session *shell* :results output :exports both 
cd ~/Desktop/tutos/git-annex
git annex sync
#+END_SRC

#+RESULTS:
#+begin_example

commit 
On branch master
nothing to commit, working tree clean
ok
pull usb 
warning: no common commits
remote: Counting objects: 5, done.
(1/3)           remote: Compressing objects:  66% (2/3)           remote: Compressing objects: 100% (3/3)           remote: Compressing objects: 100% (3/3), done.        
remote: Total 5 (delta 1), reused 0 (delta 0)
(1/5)   Unpacking objects:  40% (2/5)   Unpacking objects:  60% (3/5)   Unpacking objects:  80% (4/5)   Unpacking objects: 100% (5/5)   Unpacking objects: 100% (5/5), done.
From /media/nubudi/Ubuntu 18.04.3 LTS amd641/git-annex-usb
usb/git-annex
ok
pull testusb 
ok
(merging usb/git-annex into git-annex...)
(recording state in git...)
push usb 
Counting objects: 26, done.
Delta compression using up to 4 threads.
(1/22)   Compressing objects:   9% (2/22)   Compressing objects:  13% (3/22)   Compressing objects:  18% (4/22)   Compressing objects:  22% (5/22)   Compressing objects:  27% (6/22)   Compressing objects:  31% (7/22)   Compressing objects:  36% (8/22)   Compressing objects:  40% (9/22)   Compressing objects:  45% (10/22)   Compressing objects:  50% (11/22)   Compressing objects:  54% (12/22)   Compressing objects:  59% (13/22)   Compressing objects:  63% (14/22)   Compressing objects:  68% (15/22)   Compressing objects:  72% (16/22)   Compressing objects:  77% (17/22)   Compressing objects:  81% (18/22)   Compressing objects:  86% (19/22)   Compressing objects:  90% (20/22)   Compressing objects:  95% (21/22)   Compressing objects: 100% (22/22)   Compressing objects: 100% (22/22), done.
(1/26)   Writing objects:   7% (2/26)   Writing objects:  11% (3/26)   Writing objects:  15% (4/26)   Writing objects:  19% (5/26)   Writing objects:  23% (6/26)   Writing objects:  26% (7/26)   Writing objects:  30% (8/26)   Writing objects:  34% (9/26)   Writing objects:  38% (10/26)   Writing objects:  42% (11/26)   Writing objects:  46% (12/26)   Writing objects:  50% (13/26)   Writing objects:  53% (14/26)   Writing objects:  57% (15/26)   Writing objects:  61% (16/26)   Writing objects:  65% (17/26)   Writing objects:  69% (18/26)   Writing objects:  73% (19/26)   Writing objects:  76% (20/26)   Writing objects:  80% (21/26)   Writing objects:  84% (22/26)   Writing objects:  88% (23/26)   Writing objects:  92% (24/26)   Writing objects:  96% (25/26)   Writing objects: 100% (26/26)   Writing objects: 100% (26/26), 2.50 KiB | 853.00 KiB/s, done.
Total 26 (delta 5), reused 0 (delta 0)
To /media/nubudi/Ubuntu 18.04.3 LTS amd641/git-annex-usb
synced/git-annex
synced/master
ok
push testusb 
Counting objects: 8, done.
Delta compression using up to 4 threads.
(1/6)   Compressing objects:  33% (2/6)   Compressing objects:  50% (3/6)   Compressing objects:  66% (4/6)   Compressing objects:  83% (5/6)   Compressing objects: 100% (6/6)   Compressing objects: 100% (6/6), done.
(1/8)   Writing objects:  25% (2/8)   Writing objects:  37% (3/8)   Writing objects:  50% (4/8)   Writing objects:  62% (5/8)   Writing objects:  75% (6/8)   Writing objects:  87% (7/8)   Writing objects: 100% (8/8)   Writing objects: 100% (8/8), 873 bytes | 436.00 KiB/s, done.
Total 8 (delta 1), reused 0 (delta 0)
To /media/nubudi/Ubuntu 18.04.3 LTS amd641
synced/git-annex
ok
#+end_example


#+BEGIN_SRC shell :session *shell* :results output :exports both 
cd "/media/nubudi/Ubuntu 18.04.3 LTS amd641/git-annex-usb"
git annex sync
#+END_SRC

#+RESULTS:
#+begin_example

commit ok
merge synced/master 
Merge made by the 'recursive' strategy.
 AirPollutionInGrenoble.pdf | 1 +
 1 file changed, 1 insertion(+)
 create mode 120000 AirPollutionInGrenoble.pdf
ok
pull testlaptop 
From /home/nubudi/Desktop/tutos/git-annex
testlaptop/git-annex
testlaptop/master
testlaptop/synced/master
ok
push testlaptop 
Counting objects: 4, done.
Delta compression using up to 4 threads.
(1/3)   Compressing objects:  66% (2/3)   Compressing objects: 100% (3/3)   Compressing objects: 100% (3/3), done.
(2/4)   Writing objects:  75% (3/4)   Writing objects: 100% (4/4)   Writing objects: 100% (4/4), 470 bytes | 470.00 KiB/s, done.
Total 4 (delta 2), reused 0 (delta 0)
To /home/nubudi/Desktop/tutos/git-annex
synced/master
synced/git-annex
ok
#+end_example

We can see that the file that we had in the first repository appears
as if it has been added in the second repository. However, this isn't
the case since git-annex in this case only copied the metadata (and
now the entire content of the file).
If you want to get the content of the file, you can use the command
get.

#+BEGIN_SRC shell :session *shell* :results output :exports both 
cd /home/nubudi/Desktop/tutos/git-annex
echo "this is a test" > test.txt
git add .
git commit -m "added new file and tried to delete a second one"
#+END_SRC


#+BEGIN_SRC shell :session *shell* :results output :exports both 
cd "/media/nubudi/Ubuntu 18.04.3 LTS amd641/git-annex-usb"
git annex sync --content
#+END_SRC

#+RESULTS:
: 
: commit (recording state in git...)
: ok
: pull testlaptop 
: ok


#+BEGIN_SRC shell :session *shell* :results output :exports both
git annex whereis test.txt
#+END_SRC


 https://writequit.org/articles/getting-started-with-git-annex.html

 
* Implementing a new remote: Zenodo.
** 
