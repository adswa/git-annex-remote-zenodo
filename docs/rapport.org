#+TITLE:       rapport stage
#+AUTHOR:      Oumaima Hajji
#+STARTUP: overview indent inlineimages logdrawer
#+TAGS: R(R) Python(p) OrgMode(O) 



* Plan.
** Focalisation.
*** points (3 pages).
  - En France, il y a un mouvement autour de la science ouverte (def v)
   https://www.enseignementsup-recherche.gouv.fr/cid132529/le-plan-national-pour-la-science-ouverte-les-resultats-de-la-recherche-scientifique-ouverts-a-tous-sans-entrave-sans-delai-sans-paiement.html
  - la science ouverte: "c’est la diffusion sans entrave des
    publications et des données de la recherche.
    Son objectif : faire sortir la recherche financée sur fonds
    publics du cadre confiné des bases de données fermées. Elle réduit
    les efforts dupliqués dans la collecte, la création, le transfert
    et la réutilisation du matériel scientifique. Elle augmente ainsi
    l’efficacité de la recherche."
  - Le plan national pour la science ouverte: "rend obligatoire
    l’accès ouvert pour les publications et pour les données issues de
    recherches financées sur projets. Il met en place un Comité pour
    la science ouverte et soutient des initiatives majeures de
    structuration du paysage concernant les publications et les
    données."
  - open science, open peer review, open protocol (mooc 1: focalisé
    sur la prise des notes, le document computationnal), open data.
    ("FAIR is not equal to Open: The 'A' in FAIR stands for 'Accessible
    under well defined conditions'. ... As such, while FAIR data does
    not need to be open, in order to comply with the condition of
    reusability, FAIR data are required to have a clear, preferably
    machine readable, license.":
    https://www.go-fair.org/resources/faq/ask-question-difference-fair-data-open-data/) 
  - FAIR data: https://www.go-fair.org/fair-principles/
  - on sait qu'il faut faire ça (respecter les principes de FAIR data)
    mais comment le faire -> mooc (grâce à les outils donnés).
  - Cette version du mooc se focalise sur la partie archivage data
    pour s'assurer de sa disponibilité, ... 
  
*** liens.
 - les principes de FAIR data: https://www.go-fair.org/fair-principles/
 - open science: https://fr.wikipedia.org/wiki/Science_ouverte
   "La science ouverte (open science ou open research pour les
   anglophones) est un mouvement qui cherche à rendre la recherche
   scientifique et les données qu'elle produit accessibles à tous et
   dans tous les niveaux de la société."
 - open peer review:
   https://openscience.pasteur.fr/2020/06/04/open-peer-review-un-mouvement-qui-prend-de-lampleur/
 - open protocol.
 - open data: https://fr.wikipedia.org/wiki/Donn%C3%A9es_ouvertes
   
** Etat de l'art.
*** historique / gros fichiers (1.5 page).
**** points.
    - Dans la recherche scientifique, on travaille sur des milliers de
      fichiers de données qui évoluent au cours du temps. Il faut donc
      garder une trace de chaque version de données et du code (*)
      exécuté à un instant pour pouvoir avoir une empreinte continue du
      développement de la recherche et donc pour comprendre et suivre
      tous les pas de l'expérience. C'est pour cette raison que l'on se sert
      de Git qui est le logiciel le plus utilisé pour la gestion de
      versions. 
    - Puisque c'est difficile de gérer les fichiers de grandes tailles
      sur Git on va se servir de git-annex qui fait ça sans
      enregistrer le contenu des fichiers sur Git. En effet, blablabla.
    - Mais git-annex n'est pas le seul outil existant qui permet la
      gestion des fichiers des gros fichiers. En effet, Git a une
      extension qui permet de faire juste ça: Git
      lfs. (https://www.atlassian.com/git/tutorials/git-lfs). Mais il
      y a des problèmes avec cet outil qui gênent son utilisation:
      expliquer les deux problèmes..
    - Le choix entre les deux outils est donc fait: git-annex!
**** extra notes.
***** Importance of Version Controlling Data.
   The reality is that data is only rarely invariant. For example,
   throughout a scientific project, datasets can be extended with new
   data, adapted to new naming schemes, reorganised into different
   file hierarchies, updated with new data points or modified to fix
   any errors.
   If a dataset that is the basis for computing a scientific result
   changes without version control, reproducibility can be threatened:
   results may become invalid, or scripts that are based on file names
   that change between versions can break.
   Therefore, version controlling data and other large files in a
   similar way to version controlling code or manuscripts can help
   ensure the reproducibility of a project and capture the provenance
   of results.
   (*) L'importance de 'Data Version Control' est évidente puisque dans
   le domaine de la recherche on gère des données de différents types
   et de grande taille et ces données peuvent changer et évoluer
   rapidement et donc au final on aura plusieurs version de chaque
   fichier et quand on compile les fichiers sources on peut avoir des
   différents résultats si on prend les mauvaises versions des
   fichiers. Mais on peut pas toujours tout mettre sur Git (surtout
   dans le cas des fichiers de grande taille qui évoluent
   exponentiellement). On gère ça alors with git-annex qui crée un
   directory annex où sont stockés les noms et les métadata des
   fichiers. Donc quand le dépôt est push sur Github, seuls les
   métadata sont transmises et alors les dépôts ne sont plus
   lourds. Les données peuvent être transmises sur des autres depôts
   (figshare, amazon, ...) et on peut facilement les récupérer avec
   une commande.
   
***** Challenges in version controlling data.
   When you work, share, and collaborate on large, potentially binary
   files (such as many scientific data formats), you need to think
   about ways to version control this data with specialised
   tools. This is because most version control tools - such as Git -
   are not well suited to handle large binary data. As a Git
   repository stores every version of every file that is added to it,
   large files that undergo regular modifications can inflate the size
   of a project significantly. If others try to clone your repository
   or fetch/pull to update it locally, it will take longer to do this
   if it contains larger files that have been versioned and modified.
***** Version controlling data with git-annex.
   The git-annex tool is a distributed system that can manage and
   share large files independent from a central service or
   server. git-annex manages all file content in a separate directory
   in the repository (.git/annex/objects, the so-called annex) and
   only places file names with some metadata into version control by
   Git. When a Git repository with an annex is pushed to a web-hosting
   service such as GitHub, the contents stored in the annex are not
   uploaded. Instead, they can be pushed to a storage system.
   If a repository with an annex is cloned, the clone will not
   contain the contents of all annexed files by default, but display
   only file names. This makes the repository small, even if it tracks
   hundreds of gigabytes of data, and cloning fast, while file
   contents are stored in one or more free or commercial external
   storage solutions.
https://the-turing-way.netlify.app/reproducible-research/vcs/vcs-data.html

*** archivage (2 pages).
**** points.
    - Maintenant que l'on sait comment gérer les fichiers il faut
      passer à l'autre étape importante dans ce procès qui est l'étape
      de l'archivage.
    - Expliquer que le principe de special remotes est intéressant
      puisque la gestion est déjà faite par git-annex et qu'il faut
      juste choisir l'un des remotes qui nous est pratique et après on
      peut stocker les données dessus.
    - Expliquer pourquoi on n'a pas choisi les special remotes qui
      sont déjà implémenté par git-annex:
      https://git-annex.branchable.com/special_remotes/
    - Expliquer que la solution la plus évidente est de se servir de
      ce principe pour implémenter un special remote qui répond à nos
      attentes. Mais pour faire cela il y a plusieurs plateformes
      d'archivage: zenodo(cern), figshare, nakala, .. Il faut faire une
      comparaison de ces outils pour arriver à la conclusion que
      Zenodo est l'outil le plus intéressant pour nous.
    - Mais quand on s'appuie sur Zenodo pour faire de l'archivage, on
      remarque que il y a un shortcut entre Zenodo et github où les deux
      comptes de l'utilisateur sont connectés pour lui permettre
      d'upload ses projets github directement sur Zenodo et de les
      archiver facilement. Pourquoi pas juste utiliser ce shortcut au
      lieu de passer par git et git-annex? Le problème c'est que ce
      mechanisme est personnalisé juste pour github et donc on ne peut
      pas faire cela avec des autres plateformes comme gitlab sans
      devoir passer par des biblio. Et même quand on fait ça, il y a
      toujours un problème avec le lien Zenodo-Gitlab car cette
      méthode permet juste d'upload des fichiers sur Zenodo en
      utilisant l'API et ne permet pas de faire plus que ça. Donc la
      solution la plus évidente est de commencer par git et de
      construire un chemin vers Zenodo.
    - Parler de datalad et comme quoi il y a aussi un problem là car a
      seule solution d'archivage de ce type proposée par datalad est
      d'upload des archive zip sur figshare. donc on a implémenté le
      remote zenodo pour faire ça. 

**** extra notes.
***** zenodo.
 Zenodo is a general-purpose open-access repository developed under
 the European OpenAIRE program and operated by CERN. It allows
 researchers to deposit research papers, data sets, research
 software, reports, and any other research related digital artifacts.
 - We will be using Zenodo as the database where the articles and
   research papers will be deposited at the end of the mooc. The API
   is easily accessible through Python with the use of the package
   requests which allows the use of the basic HTTP queries.
***** datalad.
- DataLad builds on top of git-annex and extends it with an
  intuitive command-line interface. It enables users to operate
  on data using familiar concepts, such as files and directories,
  while transparently managing data access and authorization with
  underlying hosting providers.
  A powerful and complete Python API is also provided to enable
  authors of data-centric applications to bring versioning and the
  fearless acquisition of data into continuous integration workflows.
- Converting an existing Git or git-annex repository into a
  DataLad dataset: 	$ datalad create -f
- DataLad only cares (knows) about two things: Datasets and
  files. A DataLad dataset is a collection of files in
  folders. And a file is the smallest unit any dataset can
  contain. Thus, a DataLad dataset has the same structure as any
  directory on your computer, and DataLad itself can be
  conceptualized as a content-management system that operates on
  the units of files.
- exporting the content of a dataset as a ZIP archive to figshare:
  Ideally figshare should be supported as a proper git annex special
  remote. Unfortunately, figshare does not support having directories,
  and can store only a flat list of files. That makes it impossible
  for any sensible publishing of complete datasets.
*** liens.
- comparing the archiving platforms: https://espacechercheurs.enpc.fr/fr/donnees-recherche-aspects-techniques
- git-annex vs lfs: https://stackoverflow.com/questions/39337586/how-do-git-lfs-and-git-annex-differ
- nakala: https://documentation.huma-num.fr/nakala/#introduction-et-presentation
- mendeley: https://data.mendeley.com/archive-process
- datalad.
- figshare.
- github to zenodo: we know that there is alink between the two which allows to archive a github repository on zenodo (this is especially useful in the case of  when a researcher wants to cite the findings they have on github but they don't have the doi, so the next step to do is to use zenodo to archive the files that are on this repository and so we get at the end the doi number which allows us to cite): https://guides.github.com/activities/citable-code/
- l'archivage gitlab -> zenodo ne gère pas les fichiers dans git LFS: https://gitlab.com/lnesi/icpp21/-/jobs/1430800588
- library allowing to archive from gitlab to zenodo. It's still in beta stages and has just been developped since there isn't one that is already there like the github direct link: https://pypi.org/project/gitlab2zenodo/
https://gitlab.com/gitlab-org/gitlab/-/issues/25587
https://github.com/zenodo/zenodo/issues/1404 !!
https://gitlab.com/gitlab-org/gitlab/-/issues/18763
** Contributions.
*** modele de donnees (1.5 page).
- Même si Zenodo paraît comme une la plateforme parfaite à utiliser comme un
  special remote de git-annex, il y a toujours un problème
  architecturel qui nous a gêné quand on a commencé la réfléxion de
  comment structurer notre remote. Quand on fait un upload Zenodo,
  puisque son infrastructure ne permet pas d'avoir des directory,
  alors le stockage se fait dans une liste des fichiers mal structurée.  
- Un autre problème est aussi le fait que dans Zenodo quand on crée un
  nouveau upload, c'est toujours un dépôt où on va mettre tous nos
  fichiers. Donc, on peut choisir des différents modèles
  d'implémentation du remote Zenodo et pour chaque modèle, la
  fréquence de création des dépôts et les fichiers qui sont dedans
  changent. Si un dépôt est créé au moment de l'initialisation du
  remote, alors toutes les opérations qui viennent seront appliquées
  sur ce dépôt, et on aura ainsi un seul dépôt pour chaque remote
  créé. Mais on fait un autre choix, où l'initialisation du remote ne
  déclenche pas la création du dépôt, et au lieu faire cela après,
  alors c'est possible d'avoir plusieurs dépôts, et alors plusieurs
  identificateurs de dépôts pour un seul remote git-annex. Or, cela
  n'est pas nécéssaire puisque l'on peut choisir les fichiers à mettre
  dans un dépôt et ceux à laisser en local, et donc avoir un seul
  dépôt par directory (l'endroit où on initialise le remote) est
  suffisant puisqu'on peut l'utiliser quand on veut pour manipuler les
  fichiers que l'on veut sans avoir des problèmes de confusion. Si
  l'utilisateur veut créer un autre dépôt Zenodo avec un remote
  git-annex dans le même endroit que le premier remote, c'est toujours
  possible d'initialiser plusieurs remotes git-annex dans la même
  directory. 
- On a aussi fait des tests pour voir s'il y a des limites imposées
  sur le nombre de fichiers possibles à mattre dans un dépôt mais il
  n'y avait pas des problèmes avec ces tests et donc c'est possible
  d'uplad des milliers de fichiers mais la taille du dépôt ne doit pas
  atteindre 50GB. C'est la seule limite imposée par Zenodo. On les a
  contacté pour s'assurer de cette hypothèse et on a eu une réponse positive.

*** implem de remote zenodo.
L'implémentation du remote Zenodo s'est faite en plusieurs étapes:
**** api rest (0.5 page).
la première partie du procès est de comprendre comment fonctionne
l'API Zenodo et de tester les fonctionnalités possibles de cette
API. Il fallait faire des tests pour chacune des requêtes HTTP pour
tester les opérations possibles Zenodo. Les opérations les plus
importantes comme la création du dépôt, l'envoi des fichiers sur le
dépôt, la suppression, et l'obtention des informations sur le dépôt et
les fichiers stockés dedans. Il y a aussi des autres opérations pour
publier le dépôt pour archiver les données (une fois un dépôt est
publié, il devient un record qui a un doi et que l'on peut citer alors
quand on veut, on ne peut donc pas supprimer un record une fois
publié. C'est comme ça que l'on garantie son existence et on le rend
accessible et trouvable depuis le doi). On peut aussi créer des
nouvelles versions d'un record avec une simple requête post vers
l'API, et c'est grâce à cette opération que l'on veut faire évoluer
ses données en gardant des traces (chaque version publiée d'un record
a son doi, et puisque l'on peut juste avoir une seule nouvelle version
à la fois, on peut s'assurer du développement des données).
**** biblio python qui implemente deja le protocol (0.5 page).
- Afin d'implémenter un remote git-annex il faut d'abord être sûr que son
program implémente bien le protocole 'external special remote' de
git-annex qui fait le lien entre git-annex et son remote externe. Les
deux bout de la communication échangent des requêtes et des réponses
durant la période de l'exécution du programme
(celui qui implémente le remote X: git-annex-remote-X). Pour ne pas
avoir des soucis de confusion des intéractions, à chaque fois l'une des
deux parties prend l'initiative en n'envoyant que des requêtes et
l'autre partie répond alors avec des reponses à ces requêtes.
- On utilise la bibliothèque *AnnexRemote* de python qui implémente la
  totalité du protocole et respecte toutes ses spécifications. Il faut
  donc juste importer cette les modèles de cette bibiliothèque que
  l'on veut utiliser dans notre programme. On définit alors une classe
  pour notre remote qui extend la classe SpecialRemote de la
  bibliothèque. Ensuite, il nous reste à implémenter les fonctions que
  l'on va utiliser pour déposer les données sur le remote et les
  manipuler.
   
**** operation principale ().
creation d'un depot, upload, check, remove, get.
**** tests.
avec les exceptions du protocol pour s'assurer que les pb de l'api
passent bien à git-annex et qu'il y a une coherence en les deux .
**** les options possibles (newversion, 
*** archiver disableremote.
- les options pour par exemple publier le fichier .json ou 

*** restaurer une archive.

*** liens.
https://developers.zenodo.org/?python#quickstart-upload
https://git-annex.branchable.com/design/external_special_remote_protocol/
https://git-annex.branchable.com/special_remotes/external/
https://github.com/Lykos153/AnnexRemote

** Evaluation: documentaion de l'ensemble du processus avec un tutorial.
    
** Méthodologie et Compétences développées.
*** comp
   - Bilan des connaissances et expériences acquises ou approfondies au
   cours de ce stage.
   - Description sur une page d'une ou deux compétences développées
     pendant le stage. Cela peut être des compétences du métier
     d'ingénieur en informatique ou aussi des compétences
     transversales au métier d'ingénieur (voir les deux fichiers excel
     attachés).

*** metho
- ex: parler du journal (application directe des éléments de la RR).

   + parler des tutos faits au début / des petits programmes écrits
        pour tester les outils (API Zenodo, tuto git-annex, tuto
        snakemake?)
+ tests
  + doc
    + reunions
- Gestion du projet: Description de la gestion de votre projet
     (cycle de vie, structuration en taches, durées estimées et
     réelles, gestion de risques …)
  
** Conclusion.
ce j'ai pas pu faire: nakala - datalad (submodules ) voir comment ça
peut s'integrer avec zenodo (ex de figshare par opposition) -
(snakemake <-> git-annex) : pb: où integrer les commandes git annex
simples (ex get) dans un workflow snakemake.

** Bibliographie.

   


* notes.                                                           :noexport:
** a inclure.
   - Ce qui a été fait: expliquer tous les choix qui ont été faits et
     pourquoi. 
   - Description circonstanciée de ce qui n’a pu être réalisé ou
     description de ce que pourrait être la suite du travail.
   
** intro.
   - Qu'est-ce que la recherche reproductible?
- L'utilité de la recherche reproductible.
- D'où le mooc: expliquer le mooc, ce qu'il apporte de plus, parler de
  l'utilité du backend pour stocker ses fichiers et de l'importance de
  garder une trace de l'état de ces fichiers / versions / lieux de
  stockage -> d'où l'utilité de git-annex pour les bien gérer. 

  +++ le plan national pour la science ouverte (nso) : open access (les
  droits d'acces) : pb pourqu qlq uni car pas tout le monde peut payer
  pour ça et donc il y a des uni qui ont l'acess à plus de docs que
  d'autres.
  - les archives ouvertes (hal - arxiv): pas de review.
  - open data: il faut aussi avoir acces aux données (c'est pas
    suffisant d'avoir acces juste au pdf) (!!! FAIR data: il faut etre sur
    que les archives sont bien la, qu'elles sont accessibles et
    trouvables ...) on sait qu'il faut faire ça mais comment le faire
    -> mooc (grâce à les outils donnés).
  - open protocol: prise de notes (details )


------

** Etat de l'art.
*** historique / gros fichiers.
      + pourquoi git? pourquoi git-annex? control de version car
        c'est le plus utilisé mais il y a un souci avec les fichiers
        qui snt de taille tres grande.
	voir git-annex vs lsf -> 2 pbs
	1 pb avec lfs c'est que l'on peut pas
        supprimer un blob puisque un blob ets partagé par plusieurs
        repo git et du coup c'est compliqué de toucher à ça car ça
        peut causer des pb -> il y a des scripts .
	on ne peut que tout supprimer (delete tout le projet et du
        coup on perd tout le contenu de github).
	> exception: bitbucket solution mais cela ne se fait pas au
        niveau de git lfs.
	> test: git lfs vers zenodo pour voir ce qui se passe au
        fichier lfs quand on push.
	> c'est un vrai pb surtout quand on gere des fichiers de
        grandes tailles.
	2 pb: un pb avec la taille car les ficheis sont stoqués 2 fois
        car il n y a pas des liens symb et donc les fichiers sont
        stockées avec leurs tailles toutes entieres et du coup il y a
        des soucis quand on gere des data set de grandes tailles.

	> > git annex is (remote.log)
*** archivage.
	> zenodo(cern), figshare, nakala, comparaison de ces outils.
	
      + github vers zenodo (permet de deposer) != gitlab vers zenodo:
        pourquoi pas git directement? au lieu de passer par github
        donc on peut juste passer de git <- git annex -> zenodo.

      + parler de datalad et comme quoi il y a aussi un problem là
          car a seule solution d'archivage de ce type proposée par
          datalad est d'upload des archive zip sur figshare. donc on a
          implémenté le remote zenodo pour faire ça.
	  


** contributions.
*** modeles de données.
- les limitations de zenodo et le fonctionnement de git-annex et donc voila ce quon a fait pour faire fonctionner le truc. -> les choix
+ Remote Zenodo: expliquer l'architecture des dépôts Zenodo et donc
  les problèmes rencontrés lors de l'implémentation du backend (les
  moments où il fallait faire un choix: key vs filename ,
  architecture, tests nombres de fichiers possibles à mettre dans un
  dépôts, ...)
  
